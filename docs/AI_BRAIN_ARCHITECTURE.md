# ğŸ§  Vezora AI Brain Architecture

Complete guide to Vezora's AI system, database, and memory management.

---

## ğŸ“Š System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VEZORA AI SYSTEM                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚  AI BRAIN   â”‚                 â”‚  MEMORY DB  â”‚
    â”‚  (LLM)      â”‚â—„â”€â”€â”€â”€Contextâ”€â”€â”€â”€â”€â”‚  (Storage)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ Gemini â”‚   â”‚ Ollama â”‚
â”‚  API   â”‚   â”‚ Local  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤– AI Brain Components

### **1. Large Language Model (LLM)**

The "thinking" part of Vezora. Generates responses, understands context, and processes requests.

**Location:** `backend/utils/`

#### **Option A: Gemini AI (Cloud-based)** â­ RECOMMENDED

**File:** `backend/utils/geminiClient.js`

**Features:**
- ğŸ§  Google's advanced AI model
- ğŸ“š Large context window (32K tokens)
- ğŸ’¬ Better conversation quality
- ğŸš€ Fast responses (cloud-based)
- ğŸ†“ Free tier: 60 req/min

**Configuration:**
```env
GEMINI_API_KEY=your_api_key_here
GEMINI_MODEL=gemini-pro
AI_PROVIDER=auto  # or 'gemini'
```

**API Key:** [Get it here](https://makersuite.google.com/app/apikey)

---

#### **Option B: Ollama (Local)**

**File:** `backend/utils/ollamaClient.js`

**Features:**
- ğŸ”’ 100% Local (offline capability)
- ğŸ†“ Completely free
- ğŸ” Privacy-focused (data stays on device)
- ğŸ’» Requires 4GB+ RAM

**Configuration:**
```env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL_NAME=phi
AI_PROVIDER=ollama
```

**Setup:**
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull a model
ollama pull phi

# Start server
ollama serve
```

---

### **2. Intelligent Routing (Auto-Fallback)**

**Location:** `backend/routes/chat.js`

Vezora automatically chooses the best AI provider:

```javascript
Priority Order:
1. Gemini (if API key is set)
2. Ollama (if running locally)
3. Fallback to other provider if primary fails
```

**Flow:**
```
User Message
     â”‚
     â–¼
Is GEMINI_API_KEY set?
     â”‚
     â”œâ”€Yesâ”€â–º Use Gemini
     â”‚           â”‚
     â”‚           â”œâ”€Successâ”€â–º Response
     â”‚           â”‚
     â”‚           â””â”€Failâ”€â–º Fallback to Ollama
     â”‚
     â””â”€Noâ”€â”€â–º Use Ollama
                 â”‚
                 â”œâ”€Successâ”€â–º Response
                 â”‚
                 â””â”€Failâ”€â–º Error
```

---

## ğŸ’¾ Database & Memory System

### **Architecture:**

```
backend/data/
â”œâ”€â”€ memory.json        â† AI's long-term memory
â”œâ”€â”€ settings.json      â† User preferences
â””â”€â”€ logs.json          â† Chat history & actions
```

---

### **1. Memory Database (`memory.json`)**

**Purpose:** Store facts, preferences, and context for the AI to "remember"

**File:** `backend/data/memory.json`

**Structure:**
```json
{
  "default": [
    {
      "id": "mem_1234567890",
      "userId": "default",
      "type": "fact",
      "content": "User prefers React over Vue",
      "confidence": 0.95,
      "metadata": {
        "source": "conversation",
        "category": "preference"
      },
      "createdAt": "2026-02-05T10:30:00.000Z",
      "updatedAt": "2026-02-05T10:30:00.000Z"
    }
  ]
}
```

**Fields:**
- `id` - Unique identifier
- `userId` - User ID (default: "default")
- `type` - fact | preference | note | reminder
- `content` - The actual memory text
- `confidence` - How sure the AI is (0.0 - 1.0)
- `metadata` - Additional context
- `createdAt` - When memory was created
- `updatedAt` - When memory was last updated

**API Endpoints:**
- `GET /api/memory` - Fetch all memories
- `POST /api/memory` - Add new memory
- `PUT /api/memory/:id` - Update memory
- `DELETE /api/memory/:id` - Delete memory

**How It's Used:**
1. User chats with AI
2. AI identifies important facts
3. Facts stored in memory.json
4. On next conversation, AI reads memory
5. AI has context from previous chats

**Example:**
```javascript
// User says: "My name is Alex"
// Memory stored:
{
  "type": "fact",
  "content": "User's name is Alex",
  "confidence": 1.0
}

// Later, user asks: "What's my name?"
// AI reads memory and responds: "Your name is Alex!"
```

---

### **2. Settings Database (`settings.json`)**

**Purpose:** Store user preferences and AI configuration

**File:** `backend/data/settings.json`

**Structure:**
```json
{
  "default": {
    "userId": "default",
    "theme": "dark-glow",
    "personality": "friendly",
    "language": "en-US",
    "voiceCallEnabled": true,
    "temperature": 0.7,
    "maxTokens": 1024,
    "voiceSpeed": 1.0,
    "voiceName": "Microsoft Zira",
    "updatedAt": "2026-02-05T10:30:00.000Z"
  }
}
```

**Fields:**
- `theme` - UI theme
- `personality` - AI response style (friendly/professional/sassy/concise)
- `language` - Interface language
- `temperature` - AI creativity (0.0-1.0)
- `maxTokens` - Max response length
- `voiceSpeed` - TTS speed
- `voiceName` - Selected TTS voice

**API Endpoints:**
- `GET /api/settings` - Fetch settings
- `PUT /api/settings` - Update settings

---

### **3. Logs Database (`logs.json`)**

**Purpose:** Track chat history and user actions

**File:** `backend/data/logs.json`

**Structure:**
```json
{
  "default": [
    {
      "id": "log_1234567890",
      "userId": "default",
      "type": "chat",
      "message": "What's the weather?",
      "response": "I don't have access to weather data...",
      "intent": "chat",
      "responseTime": 1250,
      "provider": "gemini",
      "timestamp": "2026-02-05T10:30:00.000Z"
    },
    {
      "id": "log_1234567891",
      "userId": "default",
      "type": "action",
      "action": "open_app",
      "target": "chrome",
      "success": true,
      "timestamp": "2026-02-05T10:31:00.000Z"
    }
  ]
}
```

**Log Types:**
- `chat` - Conversation messages
- `action` - App launches, file operations
- `voice` - Voice interactions
- `error` - System errors

**API Endpoints:**
- `GET /api/logs` - Fetch logs (filtered by type/date)
- `POST /api/logs` - Add log entry
- `DELETE /api/logs` - Clear logs

---

## ğŸ”„ How It All Works Together

### **Conversation Flow:**

```
1. USER SENDS MESSAGE
   â”‚
   â–¼
2. BACKEND RECEIVES MESSAGE
   â”‚
   â”œâ”€â–º Load user settings (settings.json)
   â”œâ”€â–º Load AI memory (memory.json)
   â””â”€â–º Build context
   â”‚
   â–¼
3. SEND TO AI BRAIN
   â”‚
   â”œâ”€â–º Gemini API (if available)
   â””â”€â–º or Ollama (fallback)
   â”‚
   â–¼
4. AI GENERATES RESPONSE
   â”‚
   â”œâ”€â–º Parse intent (action detection)
   â”œâ”€â–º Generate reply
   â””â”€â–º Return response
   â”‚
   â–¼
5. POST-PROCESSING
   â”‚
   â”œâ”€â–º Store in logs (logs.json)
   â”œâ”€â–º Extract new memories (memory.json)
   â””â”€â–º Execute actions (if any)
   â”‚
   â–¼
6. SEND RESPONSE TO FRONTEND
   â”‚
   â”œâ”€â–º Display message
   â”œâ”€â–º Speak response (TTS)
   â””â”€â–º Update UI
```

---

### **Memory Context Injection:**

When you chat with Vezora, it loads relevant memories:

```javascript
// Example: User asks "What's my favorite color?"

// Step 1: Load memories
const memories = await getMemory('default');
// Returns: [{ content: "User's favorite color is blue" }]

// Step 2: Build context
const messages = [
  {
    role: 'system',
    content: 'Context from memory:\nUser\'s favorite color is blue'
  },
  {
    role: 'user',
    content: 'What\'s my favorite color?'
  }
];

// Step 3: AI sees the context and responds accurately
// Response: "Your favorite color is blue!"
```

---

## ğŸ”§ Advanced Features

### **1. Intent Parsing**

**What:** Detect if user wants to perform an action

**Example:**
```
User: "open chrome"
Intent: { action: "open_app", target: "chrome", confidence: 0.95 }

User: "search for react tutorials"
Intent: { action: "search", target: "react tutorials", confidence: 0.9 }

User: "tell me a joke"
Intent: { action: "chat", target: null, confidence: 1.0 }
```

**Used For:**
- App launching
- File operations
- Web searches
- Reminders

---

### **2. Context Window Management**

**Problem:** AI has limited context (can't remember everything)

**Solution:**
- Store important facts in `memory.json`
- Load relevant memories for each conversation
- Keep token count under model limits

**Gemini:** 32K tokens (~24K words)  
**Ollama (Phi-2):** 2K tokens (~1.5K words)

---

### **3. Embeddings (Future Enhancement)**

**What:** Convert text to vectors for semantic search

**Use Case:**
```
User: "What did I say about React?"

# Instead of exact keyword match:
# Search memory using semantic similarity
# Finds: "User prefers React" even if exact phrase wasn't used
```

**Implementation:**
```javascript
// Already available in geminiClient.js
import { generateEmbedding } from '../utils/geminiClient.js';

const embedding = await generateEmbedding("User prefers React");
// Returns: [0.123, -0.456, 0.789, ...] (768 dimensions)
```

---

## ğŸ“Š Database Comparison

| Feature | JSON Files | SQLite (Future) |
|---------|------------|-----------------|
| **Setup** | âœ… Zero setup | Requires migration |
| **Speed** | âš ï¸ Slow for large data | âœ… Very fast |
| **Queries** | âŒ Manual filtering | âœ… SQL queries |
| **Scaling** | âŒ Poor (>10K entries) | âœ… Excellent |
| **Debugging** | âœ… Easy to read | âš ï¸ Needs tools |
| **Backup** | âœ… Simple copy | âš ï¸ Export needed |

**Current:** JSON (simple, works great for personal use)  
**Future:** Can migrate to SQLite (code already in `database.js`)

---

## ğŸ” Data Privacy

### **Local Storage (JSON)**
- âœ… All data stored on your device
- âœ… No cloud backups (unless you enable)
- âœ… Full control over data

### **Gemini API**
- âš ï¸ Messages sent to Google servers
- âœ… Not used to train models
- âœ… Deleted after processing
- ğŸ”’ Use Ollama for sensitive data

### **Ollama (Local)**
- âœ… 100% local processing
- âœ… No internet required
- âœ… Perfect for privacy-sensitive use

---

## ğŸ“– File Structure Summary

```
backend/
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ geminiClient.js       â† Gemini AI integration
â”‚   â”œâ”€â”€ ollamaClient.js        â† Ollama local AI
â”‚   â”œâ”€â”€ database.js            â† Database utilities
â”‚   â””â”€â”€ fileSystem.js          â† File operations
â”‚
â”œâ”€â”€ controllers/
â”‚   â”œâ”€â”€ memoryController.js    â† Memory CRUD
â”‚   â”œâ”€â”€ settingsController.js  â† Settings CRUD
â”‚   â””â”€â”€ logsController.js      â† Logs CRUD
â”‚
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ chat.js                â† Main AI endpoint
â”‚   â”œâ”€â”€ memory.js              â† Memory API
â”‚   â”œâ”€â”€ settings.js            â† Settings API
â”‚   â””â”€â”€ logs.js                â† Logs API
â”‚
â””â”€â”€ data/
    â”œâ”€â”€ memory.json            â† AI Memory
    â”œâ”€â”€ settings.json          â† User Settings
    â””â”€â”€ logs.json              â† Chat History
```

---

## ğŸš€ Getting Started

### **1. Choose Your AI Provider:**

**Option A: Gemini (Recommended)**
```bash
# Get API key from https://makersuite.google.com/app/apikey
# Add to backend/.env:
GEMINI_API_KEY=your_api_key_here
```

**Option B: Ollama (Local)**
```bash
# Install Ollama
ollama pull phi
ollama serve
```

### **2. Initialize Database:**
```bash
cd backend
npm install
npm run dev
```

Backend will auto-create `data/` folder and JSON files.

### **3. Start Chatting:**
```bash
# Frontend
npm run dev
```

Open http://localhost:5173 and start talking!

---

## ğŸ“š API Examples

### **Chat with AI:**
```bash
POST /api/chat
{
  "message": "What's the capital of France?",
  "includeMemory": true,
  "userId": "default"
}

Response:
{
  "role": "assistant",
  "content": "The capital of France is Paris.",
  "provider": "gemini",
  "model": "gemini-pro",
  "responseTime": 1200
}
```

### **Add Memory:**
```bash
POST /api/memory
{
  "content": "User loves pizza",
  "type": "preference",
  "confidence": 1.0
}
```

### **Get Logs:**
```bash
GET /api/logs?type=chat&limit=10
```

---

## ğŸ¯ Summary

| Component | Technology | Purpose |
|-----------|------------|---------|
| **AI Brain** | Gemini/Ollama | Generate responses |
| **Memory** | memory.json | Long-term context |
| **Settings** | settings.json | User preferences |
| **Logs** | logs.json | Chat history |
| **API** | Express.js | Backend server |
| **Frontend** | React + Vite | User interface |

---

**Your Vezora AI is now powered by:**
- ğŸ§  Google Gemini (cloud AI) + Ollama (local fallback)
- ğŸ’¾ JSON database for memory and logs
- ğŸ”„ Automatic context injection
- ğŸ¯ Intent parsing for actions

---

**Back to:** [Documentation Index](README.md) | [ENV Setup](ENV_SETUP_GUIDE.md)
